"""
åˆ†ætrain.tsvä¸­5ç§äººæ ¼ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
åŒ…æ‹¬ï¼šå–å€¼èŒƒå›´ï¼ˆæœ€å°å€¼ã€æœ€å¤§å€¼ï¼‰ã€å‡å€¼ã€æ–¹å·®
ä»¥åŠäººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼ˆgender, education, race, age, incomeï¼‰çš„å®Œæ•´åˆ†æ
"""
import pandas as pd
import numpy as np
from pathlib import Path
from collections import Counter
from typing import Dict, Any, Optional


def analyze_personality_stats(train_tsv_path: str = "datasets/news_personality/train.tsv"):
    """
    åˆ†ætrain.tsvä¸­5ç§äººæ ¼ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        train_tsv_path: train.tsvæ–‡ä»¶è·¯å¾„
    """
    # è¯»å–æ•°æ®
    print(f"æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶: {train_tsv_path}")
    df = pd.read_csv(train_tsv_path, sep='\t')
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    
    # 5ä¸ªäººæ ¼ç»´åº¦åˆ—å
    personality_columns = [
        'personality_conscientiousness',  # å°½è´£æ€§
        'personality_openess',            # å¼€æ”¾æ€§
        'personality_extraversion',        # å¤–å‘æ€§
        'personality_agreeableness',       # å®œäººæ€§
        'personality_stability'            # ç¨³å®šæ€§ï¼ˆç¥ç»è´¨çš„åå‘ï¼‰
    ]
    
    # ä¸­æ–‡åç§°æ˜ å°„
    chinese_names = {
        'personality_conscientiousness': 'å°½è´£æ€§ (Conscientiousness)',
        'personality_openess': 'å¼€æ”¾æ€§ (Openness)',
        'personality_extraversion': 'å¤–å‘æ€§ (Extraversion)',
        'personality_agreeableness': 'å®œäººæ€§ (Agreeableness)',
        'personality_stability': 'ç¨³å®šæ€§ (Stability)'
    }
    
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    missing_columns = [col for col in personality_columns if col not in df.columns]
    if missing_columns:
        print(f"è­¦å‘Š: ä»¥ä¸‹åˆ—ä¸å­˜åœ¨: {missing_columns}")
        return
    
    # æå–äººæ ¼ç»´åº¦æ•°æ®
    personality_df = df[personality_columns].copy()
    
    # å¤„ç†'unknown'å€¼ï¼šæ›¿æ¢ä¸ºNaN
    personality_df = personality_df.replace('unknown', pd.NA)
    
    # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
    for col in personality_columns:
        personality_df[col] = pd.to_numeric(personality_df[col], errors='coerce')
    
    # ç»Ÿè®¡æ¯ä¸ªç»´åº¦çš„æœ‰æ•ˆæ ·æœ¬æ•°
    print("\n" + "="*80)
    print("å„ç»´åº¦æœ‰æ•ˆæ ·æœ¬æ•°ç»Ÿè®¡:")
    print("="*80)
    for col in personality_columns:
        valid_count = personality_df[col].notna().sum()
        total_count = len(personality_df)
        print(f"{chinese_names[col]:30s}: {valid_count:4d} / {total_count:4d} ({valid_count/total_count*100:.1f}%)")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*80)
    print("å„ç»´åº¦ç»Ÿè®¡ä¿¡æ¯:")
    print("="*80)
    print(f"{'ç»´åº¦':<35s} {'æœ€å°å€¼':<12s} {'æœ€å¤§å€¼':<12s} {'å‡å€¼':<12s} {'æ–¹å·®':<12s} {'æ ‡å‡†å·®':<12s}")
    print("-"*80)
    
    stats_results = {}
    
    for col in personality_columns:
        # è·å–æœ‰æ•ˆæ•°æ®ï¼ˆæ’é™¤NaNï¼‰
        valid_data = personality_df[col].dropna()
        
        if len(valid_data) == 0:
            print(f"{chinese_names[col]:<35s} {'N/A':<12s} {'N/A':<12s} {'N/A':<12s} {'N/A':<12s} {'N/A':<12s}")
            continue
        
        # è®¡ç®—ç»Ÿè®¡é‡
        min_val = valid_data.min()
        max_val = valid_data.max()
        mean_val = valid_data.mean()
        var_val = valid_data.var()
        std_val = valid_data.std()
        
        # ä¿å­˜ç»“æœ
        stats_results[col] = {
            'min': min_val,
            'max': max_val,
            'mean': mean_val,
            'variance': var_val,
            'std': std_val,
            'valid_count': len(valid_data)
        }
        
        # æ‰“å°ç»“æœ
        print(f"{chinese_names[col]:<35s} {min_val:>11.4f} {max_val:>11.4f} {mean_val:>11.4f} {var_val:>11.4f} {std_val:>11.4f}")
    
    # é¢å¤–ç»Ÿè®¡ï¼šæ•´ä½“åˆ†å¸ƒ
    print("\n" + "="*80)
    print("æ•´ä½“ç»Ÿè®¡æ‘˜è¦:")
    print("="*80)
    
    all_valid_data = []
    for col in personality_columns:
        valid_data = personality_df[col].dropna()
        all_valid_data.extend(valid_data.tolist())
    
    if all_valid_data:
        all_array = np.array(all_valid_data)
        print(f"æ‰€æœ‰ç»´åº¦åˆå¹¶åçš„ç»Ÿè®¡:")
        print(f"  æœ€å°å€¼: {all_array.min():.4f}")
        print(f"  æœ€å¤§å€¼: {all_array.max():.4f}")
        print(f"  å‡å€¼: {all_array.mean():.4f}")
        print(f"  æ–¹å·®: {all_array.var():.4f}")
        print(f"  æ ‡å‡†å·®: {all_array.std():.4f}")
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_file = "personality_stats.txt"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("äººæ ¼ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯\n")
        f.write("="*80 + "\n\n")
        f.write(f"æ•°æ®æ–‡ä»¶: {train_tsv_path}\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(df)}\n\n")
        
        f.write("å„ç»´åº¦ç»Ÿè®¡ä¿¡æ¯:\n")
        f.write("-"*80 + "\n")
        f.write(f"{'ç»´åº¦':<35s} {'æœ€å°å€¼':<12s} {'æœ€å¤§å€¼':<12s} {'å‡å€¼':<12s} {'æ–¹å·®':<12s} {'æ ‡å‡†å·®':<12s}\n")
        f.write("-"*80 + "\n")
        
        for col in personality_columns:
            if col in stats_results:
                stats = stats_results[col]
                f.write(f"{chinese_names[col]:<35s} {stats['min']:>11.4f} {stats['max']:>11.4f} "
                        f"{stats['mean']:>11.4f} {stats['variance']:>11.4f} {stats['std']:>11.4f}\n")
        
        f.write("\n" + "="*80 + "\n")
        f.write("è¯¦ç»†ç»Ÿè®¡:\n")
        f.write("="*80 + "\n")
        for col in personality_columns:
            if col in stats_results:
                stats = stats_results[col]
                f.write(f"\n{chinese_names[col]}:\n")
                f.write(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {stats['valid_count']}\n")
                f.write(f"  å–å€¼èŒƒå›´: [{stats['min']:.4f}, {stats['max']:.4f}]\n")
                f.write(f"  å‡å€¼: {stats['mean']:.4f}\n")
                f.write(f"  æ–¹å·®: {stats['variance']:.4f}\n")
                f.write(f"  æ ‡å‡†å·®: {stats['std']:.4f}\n")
    
    print(f"\nç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return stats_results


def analyze_demographic_features(
    train_tsv_path: str = "datasets/news_personality/train.tsv"
) -> Dict[str, Any]:
    """
    åˆ†æäººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼ˆgender, education, race, age, incomeï¼‰
    
    Args:
        train_tsv_path: train.tsvæ–‡ä»¶è·¯å¾„
    
    Returns:
        åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸
    """
    # è¯»å–æ•°æ®
    print(f"\n{'='*80}")
    print("äººå£ç»Ÿè®¡å­¦ç‰¹å¾åˆ†æ")
    print(f"{'='*80}")
    print(f"æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶: {train_tsv_path}")
    df = pd.read_csv(train_tsv_path, sep='\t')
    print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    
    # å®šä¹‰å­—æ®µåç§°ï¼ˆç”¨äºè¾“å‡ºï¼‰
    field_mapping = {
        'gender': {
            'name': 'æ€§åˆ« (Gender)'
        },
        'education': {
            'name': 'æ•™è‚²ç¨‹åº¦ (Education)'
        },
        'race': {
            'name': 'ç§æ— (Race)'
        },
        'age': {
            'name': 'å¹´é¾„ (Age)'
        },
        'income': {
            'name': 'æ”¶å…¥ (Income)'
        }
    }
    
    # 5ä¸ªäººæ ¼ç»´åº¦åˆ—å
    personality_columns = [
        'personality_conscientiousness',
        'personality_openess',
        'personality_extraversion',
        'personality_agreeableness',
        'personality_stability'
    ]
    
    results = {}
    
    # åˆ†ææ¯ä¸ªå­—æ®µ
    for field in ['gender', 'education', 'race', 'age', 'income']:
        print(f"\n{'-'*80}")
        print(f"ã€{field_mapping[field]['name']}ã€‘")
        print(f"{'-'*80}")
        
        if field in ['age', 'income']:
            # æ•°å€¼å‹å­—æ®µåˆ†æ
            field_data = df[field].copy()
            
            # å¤„ç†unknownå€¼
            field_data = field_data.replace('unknown', pd.NA)
            field_data = pd.to_numeric(field_data, errors='coerce')
            
            # åŸºæœ¬ç»Ÿè®¡
            valid_count = field_data.notna().sum()
            missing_count = field_data.isna().sum()
            valid_rate = valid_count / len(df) * 100
            
            print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
            print(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {valid_count} / {len(df)} ({valid_rate:.1f}%)")
            print(f"  ç¼ºå¤±æ ·æœ¬æ•°: {missing_count} ({100-valid_rate:.1f}%)")
            
            if valid_count > 0:
                print(f"\nğŸ“ˆ æ•°å€¼ç»Ÿè®¡:")
                print(f"  æœ€å°å€¼: {field_data.min():.2f}")
                print(f"  æœ€å¤§å€¼: {field_data.max():.2f}")
                print(f"  å‡å€¼: {field_data.mean():.2f}")
                print(f"  ä¸­ä½æ•°: {field_data.median():.2f}")
                print(f"  æ ‡å‡†å·®: {field_data.std():.2f}")
                print(f"  25%åˆ†ä½æ•°: {field_data.quantile(0.25):.2f}")
                print(f"  75%åˆ†ä½æ•°: {field_data.quantile(0.75):.2f}")
                
                # åˆ†ç»„ç»Ÿè®¡ï¼ˆå¦‚æœæ˜¯ageï¼ŒæŒ‰å¹´é¾„æ®µï¼›å¦‚æœæ˜¯incomeï¼ŒæŒ‰æ”¶å…¥åŒºé—´ï¼‰
                if field == 'age':
                    print(f"\nğŸ“‹ å¹´é¾„æ®µåˆ†å¸ƒ:")
                    age_bins = [0, 25, 30, 35, 40, 45, 50, 100]
                    age_labels = ['<25', '25-29', '30-34', '35-39', '40-44', '45-49', '50+']
                    age_groups = pd.cut(field_data, bins=age_bins, labels=age_labels, include_lowest=True)
                    age_counts = age_groups.value_counts().sort_index()
                    for label, count in age_counts.items():
                        pct = count / valid_count * 100
                        print(f"  {label:8s}: {count:4d} ({pct:5.1f}%)")
                
                elif field == 'income':
                    print(f"\nğŸ“‹ æ”¶å…¥åŒºé—´åˆ†å¸ƒ:")
                    income_bins = [0, 30000, 50000, 70000, 90000, 120000, float('inf')]
                    income_labels = ['<30K', '30K-50K', '50K-70K', '70K-90K', '90K-120K', '120K+']
                    income_groups = pd.cut(field_data, bins=income_bins, labels=income_labels, include_lowest=True)
                    income_counts = income_groups.value_counts().sort_index()
                    for label, count in income_counts.items():
                        pct = count / valid_count * 100
                        print(f"  {label:10s}: {count:4d} ({pct:5.1f}%)")
            
            results[field] = {
                'type': 'numeric',
                'valid_count': valid_count,
                'missing_count': missing_count,
                'stats': {
                    'min': float(field_data.min()) if valid_count > 0 else None,
                    'max': float(field_data.max()) if valid_count > 0 else None,
                    'mean': float(field_data.mean()) if valid_count > 0 else None,
                    'median': float(field_data.median()) if valid_count > 0 else None,
                    'std': float(field_data.std()) if valid_count > 0 else None,
                } if valid_count > 0 else None
            }
            
        else:
            # åˆ†ç±»å‹å­—æ®µåˆ†æ
            field_data = df[field].copy()
            
            # å¤„ç†unknownå€¼
            field_data = field_data.replace('unknown', 'unknown')
            
            # ç»Ÿè®¡é¢‘æ•°
            value_counts = field_data.value_counts().sort_index()
            total_count = len(field_data)
            
            print(f"\nğŸ“Š åˆ†å¸ƒç»Ÿè®¡:")
            print(f"{'å€¼':<20s} {'é¢‘æ•°':<10s} {'ç™¾åˆ†æ¯”':<10s} {'ç´¯è®¡ç™¾åˆ†æ¯”':<10s}")
            print(f"{'-'*50}")
            
            cumulative = 0
            distribution = {}
            for value, count in value_counts.items():
                pct = count / total_count * 100
                cumulative += pct
                # ç›´æ¥æ˜¾ç¤ºåŸå§‹å€¼ï¼Œä¸åšä»»ä½•æ˜ å°„
                value_label = str(value)
                print(f"{value_label:<20s} {count:>8d} {pct:>8.1f}% {cumulative:>8.1f}%")
                distribution[str(value)] = {'count': int(count), 'percentage': float(pct)}
            
            # ç¼ºå¤±å€¼ç»Ÿè®¡
            unknown_count = (field_data == 'unknown').sum() if 'unknown' in value_counts else 0
            valid_count = total_count - unknown_count
            
            print(f"\nğŸ“ˆ æ±‡æ€»:")
            print(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {valid_count} / {total_count} ({valid_count/total_count*100:.1f}%)")
            print(f"  ç¼ºå¤±/æœªçŸ¥: {unknown_count} ({unknown_count/total_count*100:.1f}%)")
            
            results[field] = {
                'type': 'categorical',
                'valid_count': valid_count,
                'missing_count': unknown_count,
                'distribution': distribution
            }
    
    # åˆ†æå„å­—æ®µä¸äººæ ¼ç‰¹å¾çš„å…³ç³»
    print(f"\n{'='*80}")
    print("äººå£ç»Ÿè®¡å­¦ç‰¹å¾ä¸äººæ ¼ç‰¹å¾çš„å…³ç³»åˆ†æ")
    print(f"{'='*80}")
    
    # å‡†å¤‡äººæ ¼ç‰¹å¾æ•°æ®
    personality_df = df[personality_columns].copy()
    personality_df = personality_df.replace('unknown', pd.NA)
    for col in personality_columns:
        personality_df[col] = pd.to_numeric(personality_df[col], errors='coerce')
    
    # åˆ†ææ¯ä¸ªå­—æ®µä¸äººæ ¼ç‰¹å¾çš„å…³ç³»
    for field in ['gender', 'education', 'race', 'age', 'income']:
        print(f"\n{'-'*80}")
        print(f"ã€{field_mapping[field]['name']} ä¸äººæ ¼ç‰¹å¾çš„å…³ç³»ã€‘")
        print(f"{'-'*80}")
        
        field_data = df[field].copy()
        field_data = field_data.replace('unknown', pd.NA)
        
        if field in ['age', 'income']:
            # æ•°å€¼å‹ï¼šè®¡ç®—ç›¸å…³ç³»æ•°
            field_data = pd.to_numeric(field_data, errors='coerce')
            
            print(f"\nğŸ“Š ç›¸å…³ç³»æ•° (Pearson):")
            print(f"{'äººæ ¼ç»´åº¦':<35s} {'ç›¸å…³ç³»æ•°':<12s} {'è§£é‡Š':<20s}")
            print(f"{'-'*70}")
            
            for col in personality_columns:
                # è®¡ç®—ç›¸å…³ç³»æ•°ï¼ˆåªä½¿ç”¨ä¸¤ä¸ªå­—æ®µéƒ½æœ‰æ•ˆçš„æ ·æœ¬ï¼‰
                valid_mask = field_data.notna() & personality_df[col].notna()
                if valid_mask.sum() > 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆæ ·æœ¬
                    corr = field_data[valid_mask].corr(personality_df[col][valid_mask])
                    # è§£é‡Šç›¸å…³æ€§å¼ºåº¦
                    abs_corr = abs(corr)
                    if abs_corr < 0.1:
                        strength = "æå¼±"
                    elif abs_corr < 0.3:
                        strength = "å¼±"
                    elif abs_corr < 0.5:
                        strength = "ä¸­ç­‰"
                    elif abs_corr < 0.7:
                        strength = "å¼º"
                    else:
                        strength = "æå¼º"
                    
                    print(f"{col:<35s} {corr:>11.4f} {strength:<20s}")
                else:
                    print(f"{col:<35s} {'N/A':<12s} {'æ ·æœ¬ä¸è¶³':<20s}")
        
        else:
            # åˆ†ç±»å‹ï¼šæŒ‰ç»„ç»Ÿè®¡å‡å€¼
            field_data = field_data.astype(str)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿åˆ†ç»„
            
            print(f"\nğŸ“Š å„åˆ†ç»„çš„äººæ ¼ç‰¹å¾å‡å€¼:")
            
            # è·å–æ‰€æœ‰æœ‰æ•ˆå€¼ï¼ˆæ’é™¤unknownå’ŒNaNï¼‰
            valid_values = field_data[field_data.notna() & (field_data != 'unknown') & (field_data != 'nan')].unique()
            # æ’åºï¼šå°è¯•è½¬æ¢ä¸ºæ•°å­—æ’åºï¼Œå¦åˆ™æŒ‰å­—ç¬¦ä¸²æ’åº
            try:
                valid_values = sorted([v for v in valid_values if v], key=lambda x: float(x) if str(x).replace('.', '').isdigit() else 0)
            except:
                valid_values = sorted([v for v in valid_values if v])
            
            if len(valid_values) > 0:
                # è¡¨å¤´
                header = f"{'åˆ†ç»„':<20s}"
                for col in personality_columns:
                    header += f" {col.split('_')[-1][:8]:<12s}"
                print(header)
                print(f"{'-'*80}")
                
                # æ¯ä¸ªåˆ†ç»„
                for value in valid_values:
                    mask = (field_data == str(value))
                    if mask.sum() > 0:
                        # ç›´æ¥æ˜¾ç¤ºåŸå§‹å€¼ï¼Œä¸åšä»»ä½•æ˜ å°„
                        value_label = str(value)
                        row = f"{value_label:<20s}"
                        
                        for col in personality_columns:
                            group_data = personality_df[col][mask]
                            valid_group_data = group_data.dropna()
                            if len(valid_group_data) > 0:
                                mean_val = valid_group_data.mean()
                                row += f" {mean_val:>11.4f}"
                            else:
                                row += f" {'N/A':>11s}"
                        print(row)
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_file = "demographic_stats.txt"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("äººå£ç»Ÿè®¡å­¦ç‰¹å¾åˆ†ææŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")
        f.write(f"æ•°æ®æ–‡ä»¶: {train_tsv_path}\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(df)}\n\n")
        
        # å†™å…¥è¯¦ç»†ç»“æœ
        for field in ['gender', 'education', 'race', 'age', 'income']:
            f.write("\n" + "="*80 + "\n")
            f.write(f"{field_mapping[field]['name']}\n")
            f.write("="*80 + "\n\n")
            
            if field in ['age', 'income']:
                stats = results[field]['stats']
                if stats:
                    f.write(f"æœ‰æ•ˆæ ·æœ¬æ•°: {results[field]['valid_count']}\n")
                    f.write(f"ç¼ºå¤±æ ·æœ¬æ•°: {results[field]['missing_count']}\n\n")
                    f.write("æ•°å€¼ç»Ÿè®¡:\n")
                    f.write(f"  æœ€å°å€¼: {stats['min']:.2f}\n")
                    f.write(f"  æœ€å¤§å€¼: {stats['max']:.2f}\n")
                    f.write(f"  å‡å€¼: {stats['mean']:.2f}\n")
                    f.write(f"  ä¸­ä½æ•°: {stats['median']:.2f}\n")
                    f.write(f"  æ ‡å‡†å·®: {stats['std']:.2f}\n")
            else:
                f.write(f"æœ‰æ•ˆæ ·æœ¬æ•°: {results[field]['valid_count']}\n")
                f.write(f"ç¼ºå¤±æ ·æœ¬æ•°: {results[field]['missing_count']}\n\n")
                f.write("åˆ†å¸ƒç»Ÿè®¡:\n")
                for value, info in results[field]['distribution'].items():
                    # ç›´æ¥æ˜¾ç¤ºåŸå§‹å€¼ï¼Œä¸åšä»»ä½•æ˜ å°„
                    value_label = str(value)
                    f.write(f"  {value_label}: {info['count']} ({info['percentage']:.1f}%)\n")
    
    print(f"\n{'='*80}")
    print(f"âœ… äººå£ç»Ÿè®¡å­¦ç‰¹å¾åˆ†æå®Œæˆï¼")
    print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"{'='*80}")
    
    return results


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='åˆ†ætrain.tsvä¸­5ç§äººæ ¼ç‰¹å¾å’Œäººå£ç»Ÿè®¡å­¦ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--train_file', type=str, 
                       default='datasets/news_personality/train.tsv',
                       help='train.tsvæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-demographic', action='store_true',
                       help='è·³è¿‡äººå£ç»Ÿè®¡å­¦ç‰¹å¾åˆ†æ')
    
    args = parser.parse_args()
    
    # åˆ†æäººæ ¼ç‰¹å¾
    stats = analyze_personality_stats(args.train_file)
    
    # åˆ†æäººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    if not args.no_demographic:
        demographic_stats = analyze_demographic_features(args.train_file)

